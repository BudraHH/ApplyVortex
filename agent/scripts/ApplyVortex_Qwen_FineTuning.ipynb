{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üöÄ ApplyVortex Qwen 2.5-7B Fine-Tuning\n",
                "\n",
                "This notebook fine-tunes **Qwen 2.5-7B-Instruct** on the ApplyVortex dataset for:\n",
                "- **Resume Parsing** (messy text ‚Üí structured JSON)\n",
                "- **Job Scoring** (profile + JD ‚Üí match score with reasoning)\n",
                "- **Resume Tailoring** (profile + JD ‚Üí formatted resume)\n",
                "\n",
                "### Requirements\n",
                "- **Runtime**: GPU (T4 free tier works, A100 recommended)\n",
                "- **Dataset**: `applyvortex_qwen_train.jsonl` (upload when prompted)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£ Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%capture\n",
                "!pip install unsloth\n",
                "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2Ô∏è‚É£ Upload Dataset\n",
                "\n",
                "Upload your `applyvortex_qwen_train.jsonl` file when the file picker appears."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "# Check if dataset already exists\n",
                "DATASET_PATH = \"applyvortex_qwen_train.jsonl\"\n",
                "\n",
                "if not os.path.exists(DATASET_PATH):\n",
                "    print(\"üìÇ Please upload your dataset file...\")\n",
                "    uploaded = files.upload()\n",
                "    if DATASET_PATH not in uploaded:\n",
                "        # Rename first uploaded file\n",
                "        for name in uploaded.keys():\n",
                "            os.rename(name, DATASET_PATH)\n",
                "            break\n",
                "    print(f\"‚úÖ Dataset uploaded: {DATASET_PATH}\")\n",
                "else:\n",
                "    print(f\"‚úÖ Dataset already exists: {DATASET_PATH}\")\n",
                "\n",
                "# Quick validation\n",
                "import json\n",
                "with open(DATASET_PATH, 'r') as f:\n",
                "    sample = json.loads(f.readline())\n",
                "    print(f\"\\nüìä Sample keys: {sample.keys()}\")\n",
                "    print(f\"üìù First message role: {sample['messages'][0]['role']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3Ô∏è‚É£ Load Base Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from unsloth import FastLanguageModel\n",
                "import torch\n",
                "\n",
                "# Configuration\n",
                "MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct\"\n",
                "MAX_SEQ_LENGTH = 8192  # Extended context for long CVs and JDs\n",
                "DTYPE = None  # Auto-detect (Float16 vs Bfloat16)\n",
                "LOAD_IN_4BIT = True  # 4-bit quantization for memory efficiency\n",
                "\n",
                "print(f\"üîß Loading {MODEL_NAME}...\")\n",
                "model, tokenizer = FastLanguageModel.from_pretrained(\n",
                "    model_name=MODEL_NAME,\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dtype=DTYPE,\n",
                "    load_in_4bit=LOAD_IN_4BIT,\n",
                ")\n",
                "print(\"‚úÖ Model loaded successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4Ô∏è‚É£ Configure LoRA Adapters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# LoRA Configuration\n",
                "# r=64: Higher rank for deep behavioral adaptation (complex reasoning tasks)\n",
                "# Target all key modules for full-network plasticity\n",
                "\n",
                "model = FastLanguageModel.get_peft_model(\n",
                "    model,\n",
                "    r=64,  # Higher rank for nuanced skill-requirement correlations\n",
                "    target_modules=[\n",
                "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
                "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
                "    ],\n",
                "    lora_alpha=16,\n",
                "    lora_dropout=0,  # Optimized for Unsloth\n",
                "    bias=\"none\",\n",
                "    use_gradient_checkpointing=\"unsloth\",  # For very long context\n",
                "    random_state=3407,\n",
                "    use_rslora=False,\n",
                "    loftq_config=None,\n",
                ")\n",
                "\n",
                "print(\"‚úÖ LoRA adapters configured!\")\n",
                "print(f\"üìä Trainable parameters: {model.print_trainable_parameters()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5Ô∏è‚É£ Prepare Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from datasets import load_dataset\n",
                "from unsloth.chat_templates import get_chat_template\n",
                "\n",
                "# Configure ChatML template (Qwen's native format)\n",
                "tokenizer = get_chat_template(\n",
                "    tokenizer,\n",
                "    chat_template=\"chatml\",\n",
                "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
                ")\n",
                "\n",
                "def formatting_prompts_func(examples):\n",
                "    \"\"\"Applies ChatML template to the batch.\"\"\"\n",
                "    convos = examples[\"messages\"]\n",
                "    texts = [\n",
                "        tokenizer.apply_chat_template(convo, tokenize=False, add_generation_prompt=False)\n",
                "        for convo in convos\n",
                "    ]\n",
                "    return {\"text\": texts}\n",
                "\n",
                "print(\"üìÇ Loading dataset...\")\n",
                "dataset = load_dataset(\"json\", data_files=DATASET_PATH, split=\"train\")\n",
                "\n",
                "# Validate structure\n",
                "if \"messages\" not in dataset.column_names:\n",
                "    raise KeyError(\"Dataset missing 'messages' column!\")\n",
                "\n",
                "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
                "print(f\"‚úÖ Dataset ready! {len(dataset)} samples loaded.\")\n",
                "\n",
                "# Preview a sample\n",
                "print(\"\\n--- Sample Preview ---\")\n",
                "print(dataset[0][\"text\"][:500] + \"...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6Ô∏è‚É£ Configure Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from trl import SFTTrainer\n",
                "from transformers import TrainingArguments\n",
                "\n",
                "trainer = SFTTrainer(\n",
                "    model=model,\n",
                "    tokenizer=tokenizer,\n",
                "    train_dataset=dataset,\n",
                "    dataset_text_field=\"text\",\n",
                "    max_seq_length=MAX_SEQ_LENGTH,\n",
                "    dataset_num_proc=2,\n",
                "    packing=False,  # Disabled for long documents\n",
                "    args=TrainingArguments(\n",
                "        per_device_train_batch_size=2,  # Small batch for 8k context\n",
                "        gradient_accumulation_steps=4,   # Effective batch size = 8\n",
                "        warmup_ratio=0.05,\n",
                "        num_train_epochs=2,              # Full coverage of 5k dataset\n",
                "        learning_rate=5e-5,              # Conservative to preserve reasoning\n",
                "        fp16=not torch.cuda.is_bf16_supported(),\n",
                "        bf16=torch.cuda.is_bf16_supported(),\n",
                "        logging_steps=10,\n",
                "        optim=\"adamw_8bit\",              # Memory efficient optimizer\n",
                "        weight_decay=0.01,\n",
                "        lr_scheduler_type=\"cosine\",\n",
                "        seed=3407,\n",
                "        output_dir=\"outputs\",\n",
                "        report_to=\"none\",                # Disable W&B for simplicity\n",
                "    ),\n",
                ")\n",
                "\n",
                "print(\"‚úÖ Trainer configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7Ô∏è‚É£ Start Training üèãÔ∏è"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# GPU Stats before training\n",
                "gpu_stats = torch.cuda.get_device_properties(0)\n",
                "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
                "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
                "\n",
                "print(f\"üñ•Ô∏è GPU: {gpu_stats.name}\")\n",
                "print(f\"üìä Max Memory: {max_memory} GB\")\n",
                "print(f\"üìä Reserved: {start_gpu_memory} GB\")\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"üöÄ TRAINING STARTED...\")\n",
                "print(\"=\"*50 + \"\\n\")\n",
                "\n",
                "trainer_stats = trainer.train()\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"‚úÖ TRAINING COMPLETE!\")\n",
                "print(\"=\"*50)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8Ô∏è‚É£ Training Stats"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final memory stats\n",
                "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
                "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
                "used_percentage = round(used_memory / max_memory * 100, 3)\n",
                "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
                "\n",
                "print(f\"\\nüìä Training Statistics\")\n",
                "print(f\"{'='*40}\")\n",
                "print(f\"Peak Memory Used: {used_memory} GB ({used_percentage}%)\")\n",
                "print(f\"LoRA Memory Used: {used_memory_for_lora} GB ({lora_percentage}%)\")\n",
                "print(f\"Training Time: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
                "print(f\"Samples/Second: {trainer_stats.metrics['train_samples_per_second']:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9Ô∏è‚É£ Save Model to Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "\n",
                "# Mount Google Drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Save paths\n",
                "LOCAL_OUTPUT_DIR = \"ApplyVortex-Qwen2.5-7B-Adapter\"\n",
                "DRIVE_OUTPUT_DIR = \"/content/drive/MyDrive/ApplyVortex-Models/ApplyVortex-Qwen2.5-7B-Adapter\"\n",
                "\n",
                "print(f\"üíæ Saving model locally to {LOCAL_OUTPUT_DIR}...\")\n",
                "model.save_pretrained(LOCAL_OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(LOCAL_OUTPUT_DIR)\n",
                "\n",
                "print(f\"‚òÅÔ∏è Copying to Google Drive: {DRIVE_OUTPUT_DIR}...\")\n",
                "!mkdir -p \"{DRIVE_OUTPUT_DIR}\"\n",
                "!cp -r {LOCAL_OUTPUT_DIR}/* \"{DRIVE_OUTPUT_DIR}/\"\n",
                "\n",
                "print(\"‚úÖ Model saved to Google Drive!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## üîü Test the Fine-Tuned Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Enable inference mode\n",
                "FastLanguageModel.for_inference(model)\n",
                "\n",
                "# Test Resume Parsing\n",
                "test_resume = \"\"\"John Smith\n",
                "john.smith@email.com | +1-555-0123\n",
                "San Francisco, CA\n",
                "\n",
                "Senior Software Engineer with 5+ years of Python and AWS experience.\n",
                "\n",
                "EXPERIENCE\n",
                "TechCorp - Senior Software Engineer\n",
                "2021-01 - Present\n",
                "- Built microservices using Python and FastAPI\n",
                "- Reduced latency by 40% through Redis caching\n",
                "\n",
                "EDUCATION\n",
                "Stanford University - MS Computer Science\n",
                "\n",
                "SKILLS\n",
                "Python, AWS, Docker, Kubernetes, PostgreSQL\n",
                "\"\"\"\n",
                "\n",
                "messages = [\n",
                "    {\"role\": \"system\", \"content\": \"You are a specialized Resume Parsing Engine. Extract candidate data into strict ApplyVortex JSON schema. Return ONLY valid JSON.\"},\n",
                "    {\"role\": \"user\", \"content\": test_resume}\n",
                "]\n",
                "\n",
                "inputs = tokenizer.apply_chat_template(\n",
                "    messages,\n",
                "    tokenize=True,\n",
                "    add_generation_prompt=True,\n",
                "    return_tensors=\"pt\"\n",
                ").to(\"cuda\")\n",
                "\n",
                "outputs = model.generate(\n",
                "    input_ids=inputs,\n",
                "    max_new_tokens=2048,\n",
                "    use_cache=True,\n",
                "    temperature=0.1,\n",
                ")\n",
                "\n",
                "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
                "print(\"ü§ñ Model Response:\")\n",
                "print(response.split(\"assistant\")[-1] if \"assistant\" in response else response)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1Ô∏è‚É£1Ô∏è‚É£ Export to GGUF (Optional - for Ollama/llama.cpp)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Uncomment to export to GGUF format for local inference with Ollama\n",
                "\n",
                "# GGUF_OUTPUT = \"ApplyVortex-Qwen2.5-7B-Q4_K_M.gguf\"\n",
                "#\n",
                "# model.save_pretrained_gguf(\n",
                "#     GGUF_OUTPUT,\n",
                "#     tokenizer,\n",
                "#     quantization_method=\"q4_k_m\"  # Good balance of quality and size\n",
                "# )\n",
                "#\n",
                "# # Copy to Drive\n",
                "# !cp {GGUF_OUTPUT} \"/content/drive/MyDrive/ApplyVortex-Models/\"\n",
                "# print(f\"‚úÖ GGUF exported to Google Drive!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## ‚úÖ All Done!\n",
                "\n",
                "Your fine-tuned model is saved to:\n",
                "- **Google Drive**: `/MyDrive/ApplyVortex-Models/ApplyVortex-Qwen2.5-7B-Adapter/`\n",
                "\n",
                "### Next Steps:\n",
                "1. Download the adapter from Google Drive\n",
                "2. Load it in your ApplyVortex agent using `peft` or convert to GGUF for Ollama\n",
                "3. Update your agent's `config.py` to point to the new model\n",
                "\n",
                "---\n",
                "*Generated by ApplyVortex Training Pipeline*"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}